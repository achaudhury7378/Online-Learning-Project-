# Online-Learning-Project-
HyperBand Implementation
Applying machine learning (ML) and complex predictive systems requires users to set variousknobs and parameter configurations, which are broadly referred to as hyperparameters (HPs).While good performance hinges on a careful tuning of those, this process is time-consuming andcrucially depends on the level of expertise of the practitioner. ML examples of HPs are the learningrate of stochastic gradient descent algorithms or the architectural choices of deep neural networks.In order to automate the tuning of HPs, various approaches such as random search or Bayesianoptimization (BO) have successfully cast this problem as a global black-box optimization problem.In that formulation, querying the black-box function of interest, sayf, typically correspondsto a full training of the underlying ML model together with its evaluation on some validationdata. Here we discuss a specific technique of hyperparameter optimization as a non-stochasticinfinite-armed bandit problem known as the Hyperband. Hyperband relies on a successive halvingprocedure, where a pool containing randomly sampled HPs is progressively trimmed accordingto a theoretically-justified resource schedule. Despite its simplicity, Hyperband was proven tooutperform BO on many ML-related tuning tasks in regimes where solutions with moderateprecision are acceptable. In this project, we reveiwed literatures on infinite armed bandit problemsin context of Hyperparameter Optimization.We also implemented HyperBand, an improvisedversion of Successive Halving, a robust, general-purpose solutionto the Non-stochastic best armidentification problem, for finding optimal Hyperparameter configuration.
